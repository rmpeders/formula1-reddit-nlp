{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14780b04-fabd-40cf-930d-9c9eb83f9024",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57543a49-58b8-4d1a-8068-bcf8bb97e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0f7e537-4da8-4cd8-9e97-159f71ffa997",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import data\n",
    "df = pd.read_csv('../data/subreddit_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65b73f44-b29f-40c2-83b2-429053be8bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>clean_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>If the sun is reporting it then it wouldn’t be...</td>\n",
       "      <td>if the sun is reporting it then it wouldn’t be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Micheal masi's answer : 1</td>\n",
       "      <td>micheal masi's answer : 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>it's all fun and games till charles and carlos...</td>\n",
       "      <td>it's all fun and games till charles and carlos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>formuladank</td>\n",
       "      <td># #stillwerise</td>\n",
       "      <td># #stillwerise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>2.99??????? wtf</td>\n",
       "      <td>2.99??????? wtf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                               body  \\\n",
       "0  formuladank  If the sun is reporting it then it wouldn’t be...   \n",
       "1  formuladank                          Micheal masi's answer : 1   \n",
       "2  formuladank  it's all fun and games till charles and carlos...   \n",
       "3  formuladank                                     # #stillwerise   \n",
       "4  formuladank                                    2.99??????? wtf   \n",
       "\n",
       "                                          clean_body  \n",
       "0  if the sun is reporting it then it wouldn’t be...  \n",
       "1                          micheal masi's answer : 1  \n",
       "2  it's all fun and games till charles and carlos...  \n",
       "3                                     # #stillwerise  \n",
       "4                                    2.99??????? wtf  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fccb7de-b1e9-40e5-a8cd-e995eae7b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create X and y\n",
    "X = df['clean_body']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee4986c-da3e-47e5-8211-4aa1ef18f46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "formuladank    0.5\n",
       "formula1       0.5\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline (Even though we know this should be a 50/50 split)\n",
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25801d3a-c2eb-40f5-8424-88cba733fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d933225-c85c-41af-bea6-9239037bebe5",
   "metadata": {},
   "source": [
    "# Start with standard model combinations (Count Vectorizer/TF-IDF and Naive Bayes/Logistic Regression)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0314c7-9513-4693-bd0f-6ed3428e8a30",
   "metadata": {},
   "source": [
    "### CVEC and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653a0753-31aa-45b3-9f6b-512632721840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7970666666666667\n",
      "Test: 0.6526\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "pipe_cvec_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit \n",
    "pipe_cvec_nb.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print(f'Train: {pipe_cvec_nb.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe_cvec_nb.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ae315-03e8-45e6-8e1b-6aef34851f45",
   "metadata": {},
   "source": [
    "### CVEC and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75fd63a-35d3-4535-8151-6ed40b4697fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.8737333333333334\n",
      "Test: 0.6612\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "pipe_cvec_log = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('nb', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "# Fit \n",
    "pipe_cvec_log.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print(f'Train: {pipe_cvec_log.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe_cvec_log.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6445a00-3892-4ea4-9a25-5060fec07332",
   "metadata": {},
   "source": [
    "### TF-IDF and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9481ae-35e5-46cd-ab00-6b283c956bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7995333333333333\n",
      "Test: 0.6458\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "pipe_tvec_nb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit \n",
    "pipe_tvec_nb.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print(f'Train: {pipe_tvec_nb.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe_tvec_nb.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea20a69-6f87-4a17-8f7d-3fe4c4586a86",
   "metadata": {},
   "source": [
    "### TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5216027a-bdf1-4d36-8340-e323ff82d3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7951333333333334\n",
      "Test: 0.6624\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "pipe_tvec_log = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "# Fit \n",
    "pipe_tvec_log.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print(f'Train: {pipe_tvec_log.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe_tvec_log.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89663d64-6cf4-4e71-9fad-64798af517dd",
   "metadata": {},
   "source": [
    "Out of these, Logistic Regression is definitely performing better than Naive Bayes. CVEC and TF-IDF are close, however TF-IDF is giving us a test score much closer to our training score which implies it is less subject to overfitting.\n",
    "\n",
    "We also see that every model outperforms our baseline accuracy of 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836168f5-0f9a-43b3-8ec8-fbaf8955d9a9",
   "metadata": {},
   "source": [
    "#### Predictions/Confusion Matrix for our TF-IDF/Logistic Regression Model\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78df6459-a3a7-4844-9e24-1c229108d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    formula1       0.66      0.66      0.66      2500\n",
      " formuladank       0.66      0.66      0.66      2500\n",
      "\n",
      "    accuracy                           0.66      5000\n",
      "   macro avg       0.66      0.66      0.66      5000\n",
      "weighted avg       0.66      0.66      0.66      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = pipe_tvec_log.predict(X_test)\n",
    "print(classification_report(y_test, preds))\n",
    "plot_confusion_matrix(pipe_tvec_log, X_test, y_test, cmap = 'magma');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c069e-a7ba-4e96-a20d-d1c9ed6eef53",
   "metadata": {},
   "source": [
    "The model doesn't seem to be particularly favored toward false classifications either way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8f47c-b8a0-4e64-ae4c-082d004d50e8",
   "metadata": {},
   "source": [
    "## Grid Search on TF-IDF/Logistic Regression\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9ec2a1b-8092-4c10-9d8e-177d6c70e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [2_000, 3_000, 4_000, 5_000],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 2)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4eb902f-16ab-4711-ba04-3c895bd741a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('log',\n",
       "                                        LogisticRegression(max_iter=10000))]),\n",
       "             param_grid={'tvec__max_features': [2000, 3000, 4000, 5000],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': [None, 'english']})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1dc2ae6-96c1-4d36-8d41-c99e87a3ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7646\n",
      "Test: 0.6626\n",
      "Best Params: {'tvec__max_features': 5000, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618a1eb-a073-4180-a5c7-bf15612958eb",
   "metadata": {},
   "source": [
    "This gave us an ever so slightly better model. Because the max features was on the upper end, I will create a new gridsearch with a higher max features. Will also try to increase ngram to 1,3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3353afda-2968-425b-98d6-bdf69f436850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7751333333333333\n",
      "Test: 0.6628\n",
      "Best Params: {'tvec__max_features': 7000, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [5_000, 6_000, 7_000, 8_000, 9_000],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 3)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ae544-b2d4-44a9-a7fe-7795ff08887d",
   "metadata": {},
   "source": [
    "Slightly better with max features at 7000, all other params are the same. Lets add new params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b89135-5ef1-4752-bd9f-15ae2c8df404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tvec', TfidfVectorizer()),\n",
       "  ('log', LogisticRegression(max_iter=10000))],\n",
       " 'verbose': False,\n",
       " 'tvec': TfidfVectorizer(),\n",
       " 'log': LogisticRegression(max_iter=10000),\n",
       " 'tvec__analyzer': 'word',\n",
       " 'tvec__binary': False,\n",
       " 'tvec__decode_error': 'strict',\n",
       " 'tvec__dtype': numpy.float64,\n",
       " 'tvec__encoding': 'utf-8',\n",
       " 'tvec__input': 'content',\n",
       " 'tvec__lowercase': True,\n",
       " 'tvec__max_df': 1.0,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__norm': 'l2',\n",
       " 'tvec__preprocessor': None,\n",
       " 'tvec__smooth_idf': True,\n",
       " 'tvec__stop_words': None,\n",
       " 'tvec__strip_accents': None,\n",
       " 'tvec__sublinear_tf': False,\n",
       " 'tvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tvec__tokenizer': None,\n",
       " 'tvec__use_idf': True,\n",
       " 'tvec__vocabulary': None,\n",
       " 'log__C': 1.0,\n",
       " 'log__class_weight': None,\n",
       " 'log__dual': False,\n",
       " 'log__fit_intercept': True,\n",
       " 'log__intercept_scaling': 1,\n",
       " 'log__l1_ratio': None,\n",
       " 'log__max_iter': 10000,\n",
       " 'log__multi_class': 'auto',\n",
       " 'log__n_jobs': None,\n",
       " 'log__penalty': 'l2',\n",
       " 'log__random_state': None,\n",
       " 'log__solver': 'lbfgs',\n",
       " 'log__tol': 0.0001,\n",
       " 'log__verbose': 0,\n",
       " 'log__warm_start': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e555a85d-f806-4aa8-b5fc-d6ffe627a2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.6851333333333334\n",
      "Test: 0.6498\n",
      "Best Params: {'tvec__max_df': 0.5, 'tvec__max_features': 7000, 'tvec__min_df': 50}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [7_000],\n",
    "    'tvec__max_df': [.5, .75, .95],\n",
    "    'tvec__min_df': [50, 150, 400]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bdd562d-e795-4354-8f2a-669e6e7a9cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7732666666666667\n",
      "Test: 0.665\n",
      "Best Params: {'tvec__max_df': 0.25, 'tvec__max_features': 7000, 'tvec__min_df': 1}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [7_000],\n",
    "    'tvec__max_df': [.25, .3, .35, .4, .45],\n",
    "    'tvec__min_df': [1, 5, 10]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8fa98-5ba3-48f1-ab2d-87829f2e3c0b",
   "metadata": {},
   "source": [
    "Nailed down what seems to be the best parameters as far as these go. Lets move on to preprocessing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d488f78-1294-44ea-b47a-e4e037842457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for lemmatizing   (Both of the functions below were pulled from Katie Sylvia's NLP Practice lesson)\n",
    "def lemmatize_body(body):\n",
    "\n",
    "    # split into words\n",
    "    split_body = body.split()\n",
    "\n",
    "    # instantiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # lemmatize and rejoin\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in split_body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96186012-7874-4313-a986-fd1b87dac75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for stemming\n",
    "def stem_body(body):\n",
    "\n",
    "    # split into words\n",
    "    split_body = body.split()\n",
    "\n",
    "    # instantiate stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # stem and rejoin\n",
    "    return ' '.join([stemmer.stem(word) for word in split_body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a87b5ea7-49d3-456c-b5fd-cd9fb2158e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7713333333333333\n",
      "Test: 0.659\n",
      "Best Params: {'tvec__max_df': 0.25, 'tvec__max_features': 7000, 'tvec__preprocessor': <function stem_body at 0x0000017980962550>}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [7_000],\n",
    "    'tvec__max_df': [.25],\n",
    "    'tvec__preprocessor': [None, stem_body, lemmatize_body]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7850a8e4-cb6c-4756-bd14-10cc9a141353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7690666666666667\n",
      "Test: 0.6656\n",
      "Best Params: {'log__C': 0.75, 'tvec__max_df': 0.3, 'tvec__max_features': 10000, 'tvec__min_df': 1, 'tvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [100, 500, 1000, 2000, 3000, 5000, 6000, 7_000, 8000, 9000, 10000],\n",
    "    'tvec__max_df': [.1, .15, .2, .25, .3, .35, .4, .45, .5, .6, .7, .8, .9],\n",
    "    'tvec__min_df': [1, 5, 10, 25, 50],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'log__C': [1, .75, .5, .25]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2e379-9360-49c3-9d97-2eb898f12945",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forest Modeling\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0947cc1a-8d91-46a8-a64d-ef9b6cb3434d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9782\n",
      "Test: 0.647\n",
      "Best Params: {'cvec__max_df': 0.25, 'cvec__max_features': 8000, 'cvec__min_df': 1, 'cvec__stop_words': None, 'rf__max_depth': None, 'rf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [3000, 5000, 6000, 7_000, 8000],\n",
    "    'cvec__max_df': [.25, .5, .75, 1.0],\n",
    "    'cvec__min_df': [1, 10, 25, 50, 100],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'rf__n_estimators': [50, 100, 150, 200],\n",
    "    'rf__max_depth': [None, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45bbda27-5a7f-4833-8956-3f6fe508b29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9777333333333333\n",
      "Test: 0.644\n",
      "Best Params: {'cvec__max_features': 7000, 'cvec__stop_words': None, 'rf__max_depth': None, 'rf__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [3000, 5000, 6000, 7_000],\n",
    "    'cvec__stop_words': [None, 'english'],\n",
    "    'rf__n_estimators': [100, 200],\n",
    "    'rf__max_depth': [None, 5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "260c41c1-adb9-45ad-b246-993caa4b4206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7606\n",
      "Test: 0.6508\n",
      "Best Params: {'gb__learning_rate': 0.1, 'gb__max_depth': 5, 'gb__n_estimators': 200, 'tvec__max_features': 7000, 'tvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [7_000, 8000],\n",
    "    'tvec__stop_words': [None],\n",
    "    'gb__n_estimators': [200],\n",
    "    'gb__max_depth': [1, 5, 10],\n",
    "    'gb__learning_rate': [.08, .1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf5ce7-2bee-4535-b02c-cecd208a14f8",
   "metadata": {},
   "source": [
    "---- \n",
    "# Title models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c477560-40d6-4cab-85f7-ac9a82808039",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.read_csv('../data/subreddit_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30efd541-cbef-49e7-b0d1-74e25017e71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>interesting title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>F1 Cars are planes upside down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Congrats K-Mag on your IMSA Pole!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Fun fact: Valtteri Bottas is in fact ALSO the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>POV: NIKITA MAZESBIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                              title\n",
       "0  formuladank                                  interesting title\n",
       "1  formuladank                     F1 Cars are planes upside down\n",
       "2  formuladank                  Congrats K-Mag on your IMSA Pole!\n",
       "3  formuladank  Fun fact: Valtteri Bottas is in fact ALSO the ...\n",
       "4  formuladank                               POV: NIKITA MAZESBIN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a9abb71-1351-45db-9dc6-a3608c3d8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_t['title']\n",
    "y = df_t['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7437ff4e-2780-448c-a8c3-a84efb22fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655746b6-12f2-494a-9adb-774bbb37c954",
   "metadata": {},
   "source": [
    "### Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b021a93-d6a9-44e4-bd7a-219102383b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.8541333333333333\n",
      "Test: 0.7556\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "pipe_tvec_log = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('log', LogisticRegression(max_iter = 10_000))\n",
    "])\n",
    "\n",
    "# Fit \n",
    "pipe_tvec_log.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print(f'Train: {pipe_tvec_log.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe_tvec_log.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a65ef-2d20-4eda-a5ef-ea7c07160a4d",
   "metadata": {},
   "source": [
    "### Naive Bayes Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef7e6a6-758b-42fc-98a9-d462a5aef71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.8718666666666667\n",
      "Test: 0.7272\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "pipe_tvec_nb = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit \n",
    "pipe_tvec_nb.fit(X_train, y_train)\n",
    "\n",
    "# Scores\n",
    "print(f'Train: {pipe_tvec_nb.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe_tvec_nb.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd1909-9822-48cb-b204-7383efacdcba",
   "metadata": {},
   "source": [
    "### Gradient Boosting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c289151-aed0-4668-80df-cea1a67dfd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7554627696590118\n",
      "Test: 0.7178631051752922\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f'Train: {pipe.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec962e12-b27f-48b1-8823-57106a2dd920",
   "metadata": {},
   "source": [
    "### Random Forest Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc0ffe6-10c4-4cad-bece-f7df252b835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9948\n",
      "Test: 0.7696\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f'Train: {pipe.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0c6fd-beaa-47d2-88a5-ccc95b0df1a8",
   "metadata": {},
   "source": [
    "### Random Forest with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b19e796-6283-46d9-b915-a3fdf1a0b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9948\n",
      "Test: 0.754\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f'Train: {pipe.score(X_train, y_train)}')\n",
    "print(f'Test: {pipe.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2bd9bb-52a3-4d26-88c4-fd6a6d674417",
   "metadata": {},
   "source": [
    "### Random Forest TF-IDF Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e225daed-e9a9-4414-b815-31c31780ae47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9945333333333334\n",
      "Test: 0.7724\n",
      "Best Params: {'rf__max_depth': None, 'rf__n_estimators': 200, 'tvec__max_df': 0.25, 'tvec__max_features': 8000, 'tvec__min_df': 1, 'tvec__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [3000, 5000, 6000, 7_000, 8000, 10000],\n",
    "    'tvec__max_df': [.25, .5, .75, 1.0],\n",
    "    'tvec__min_df': [1, 10, 25, 50],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "    'rf__n_estimators': [50, 100, 150, 200],\n",
    "    'rf__max_depth': [None, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dec9c4-fa55-47a5-a385-d57d8a24ef8d",
   "metadata": {},
   "source": [
    "### Grid Search with lemmatize and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c3176e-4ebb-4016-94aa-65c929454923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9956\n",
      "Test: 0.7604\n",
      "Best Params: {'rf__max_depth': None, 'rf__n_estimators': 200, 'tvec__max_df': 0.25, 'tvec__max_features': 10000, 'tvec__min_df': 1, 'tvec__preprocessor': <function lemmatize_body at 0x000001658E9C9280>}\n"
     ]
    }
   ],
   "source": [
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'tvec__max_features': [3000, 5000, 6000, 7_000, 8000, 10000],\n",
    "    'tvec__max_df': [.25, .5, .75, 1.0],\n",
    "    'tvec__min_df': [1, 10, 25, 50],\n",
    "    'tvec__preprocessor': [None, lemmatize_body, stem_body],\n",
    "    'rf__n_estimators': [50, 100, 150, 200],\n",
    "    'rf__max_depth': [None, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                      param_grid = pipe_params,\n",
    "                      cv = 5,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5450de15-477a-445b-8d2d-94ccb017a912",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c89a3b-e611-4f80-9654-e1a11a21c4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>interesting title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>F1 Cars are planes upside down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Congrats K-Mag on your IMSA Pole!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Fun fact: Valtteri Bottas is in fact ALSO the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>POV: NIKITA MAZESBIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                              title\n",
       "0  formuladank                                  interesting title\n",
       "1  formuladank                     F1 Cars are planes upside down\n",
       "2  formuladank                  Congrats K-Mag on your IMSA Pole!\n",
       "3  formuladank  Fun fact: Valtteri Bottas is in fact ALSO the ...\n",
       "4  formuladank                               POV: NIKITA MAZESBIN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff12b1a5-69df-46b5-bd80-afa2407e07b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>interesting title</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>F1 Cars are planes upside down</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Congrats K-Mag on your IMSA Pole!</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Fun fact: Valtteri Bottas is in fact ALSO the ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>POV: NIKITA MAZESBIN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                              title  word_count\n",
       "0  formuladank                                  interesting title           2\n",
       "1  formuladank                     F1 Cars are planes upside down           6\n",
       "2  formuladank                  Congrats K-Mag on your IMSA Pole!           6\n",
       "3  formuladank  Fun fact: Valtteri Bottas is in fact ALSO the ...          15\n",
       "4  formuladank                               POV: NIKITA MAZESBIN           3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t['word_count'] = df_t['title'].str.split(' ').str.len()\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "357e7bda-17b3-4473-a3a8-80cc054d4755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count</th>\n",
       "      <th>title_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>interesting title</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>F1 Cars are planes upside down</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Congrats K-Mag on your IMSA Pole!</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Fun fact: Valtteri Bottas is in fact ALSO the ...</td>\n",
       "      <td>15</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>POV: NIKITA MAZESBIN</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                              title  word_count  \\\n",
       "0  formuladank                                  interesting title           2   \n",
       "1  formuladank                     F1 Cars are planes upside down           6   \n",
       "2  formuladank                  Congrats K-Mag on your IMSA Pole!           6   \n",
       "3  formuladank  Fun fact: Valtteri Bottas is in fact ALSO the ...          15   \n",
       "4  formuladank                               POV: NIKITA MAZESBIN           3   \n",
       "\n",
       "   title_length  \n",
       "0            17  \n",
       "1            30  \n",
       "2            33  \n",
       "3            84  \n",
       "4            20  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t['title_length'] = df_t['title'].str.len()\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37ca3c1b-8674-406f-800a-dc48369d7d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.997633959638135\n",
      "Test: 0.7604340567612687\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "tvec = TfidfVectorizer(max_df = .25, max_features = 8000)\n",
    "tv = tvec.fit_transform(df_t['title'])\n",
    "df_tv = pd.DataFrame(tv.todense(), columns = tvec.get_feature_names_out())\n",
    "df_tv.join(df_t[['word_count', 'title_length']])\n",
    "\n",
    "X = df_tv\n",
    "y = df_t['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "rf = RandomForestClassifier(n_estimators = 200)\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "007155a4-41b9-471e-aa6a-8287eb64c3e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "1350 fits failed out of a total of 4050.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1350 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\rmped\\miniconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.73208072 0.75421016 0.75601949 0.75908142 0.76075157 0.73444676\n",
      " 0.75281837 0.7565762  0.76102992 0.76089074 0.73695198 0.75768963\n",
      " 0.75894224 0.75643702 0.75768963 0.56423104 0.6243563  0.64314544\n",
      " 0.64648573 0.63966597 0.54697286 0.62741823 0.63284621 0.64425887\n",
      " 0.64481559 0.54613779 0.62101601 0.64258873 0.64258873 0.65539318\n",
      " 0.57995825 0.66235212 0.66903271 0.67139875 0.68643006 0.62519137\n",
      " 0.65720251 0.67153793 0.68350731 0.68392484 0.6098817  0.66068198\n",
      " 0.66137787 0.6776618  0.68058455 0.62908838 0.67752262 0.69032707\n",
      " 0.69839944 0.69617258 0.62672234 0.67585247 0.69171886 0.6921364\n",
      " 0.69742519 0.6164231  0.67974948 0.68629088 0.69241475 0.69631176\n",
      " 0.65678497 0.69311065 0.69617258 0.70438413 0.70535839 0.64384134\n",
      " 0.68545581 0.69812109 0.70048713 0.70897704 0.64370216 0.68378566\n",
      " 0.70160056 0.6993737  0.70285317 0.66304802 0.68601253 0.70508003\n",
      " 0.70535839 0.71064718 0.63827418 0.68462074 0.69951287 0.70368824\n",
      " 0.7078636  0.65720251 0.69241475 0.70466249 0.70688935 0.70772443\n",
      " 0.7348643  0.74780793 0.76005567 0.75796799 0.75768963 0.73569937\n",
      " 0.75407098 0.75643702 0.75866388 0.76033403 0.73500348 0.75699374\n",
      " 0.75810717 0.76144746 0.76200418 0.56297843 0.61266527 0.6533055\n",
      " 0.64050104 0.66026444 0.55574113 0.62421712 0.64481559 0.65010438\n",
      " 0.64300626 0.56701461 0.62616562 0.64773834 0.65302714 0.64286708\n",
      " 0.59846903 0.6578984  0.67654836 0.6796103  0.68837857 0.58649965\n",
      " 0.65414057 0.67585247 0.68169798 0.67613083 0.59443285 0.64926931\n",
      " 0.67320807 0.68044537 0.68100209 0.62004175 0.67835769 0.68573417\n",
      " 0.69074461 0.69380654 0.63131524 0.67153793 0.69436326 0.69116214\n",
      " 0.69060543 0.63103688 0.67835769 0.68517745 0.69088379 0.69380654\n",
      " 0.64022269 0.69700765 0.69060543 0.70118302 0.70201809 0.63785665\n",
      " 0.69241475 0.69923452 0.70034795 0.70299235 0.63604732 0.68740431\n",
      " 0.69227557 0.7039666  0.70563674 0.65553236 0.68768267 0.70508003\n",
      " 0.7131524  0.70967293 0.65970772 0.68754349 0.70076548 0.70800278\n",
      " 0.70688935 0.64968685 0.69004871 0.70257481 0.70730689 0.71064718\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.50201809\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.50201809 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.50173974 0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.50173974 0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.50201809 0.5032707  0.5032707  0.5032707  0.5032707\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.50201809\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.50173974\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.50201809 0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.50173974 0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.50048713 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.50201809 0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.50048713 0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.50201809\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.50201809 0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.50201809 0.5032707  0.5032707\n",
      " 0.5032707  0.5032707  0.5032707  0.5032707  0.5032707  0.5032707\n",
      " 0.5032707  0.50173974 0.5032707  0.5032707  0.5032707  0.5032707\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9942936673625609\n",
      "Test: 0.7641903171953256\n",
      "Best Params: {'ccp_alpha': 0.0, 'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_params = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rf,\n",
    "                      param_grid = grid_params,\n",
    "                      cv = 5,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(f'Train: {gs.score(X_train, y_train)}')\n",
    "print(f'Test: {gs.score(X_test, y_test)}')\n",
    "print(f'Best Params: {gs.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f499e-89d1-46df-9eea-02c828344d42",
   "metadata": {},
   "source": [
    "It looks like adding these features did not help the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748977b-ff9a-47fe-87fc-58aa175317a1",
   "metadata": {},
   "source": [
    "### Simple logreg model with just title length and word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7868cbea-08c6-43e7-acce-fb2930f95aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_simple = df_t[['title_length', 'word_count']]\n",
    "y_simple = df_t['subreddit']\n",
    "\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X_simple, y_simple, random_state=42, stratify=y_simple)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_s_train, y_s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9f142f7-1c7a-458e-9447-8cf08a5e4e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6876826722338205\n",
      "0.6832220367278798\n"
     ]
    }
   ],
   "source": [
    "print(logreg.score(X_s_train, y_s_train))\n",
    "print(logreg.score(X_s_test, y_s_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa501fc-3871-42f3-aaac-c6c197d71d28",
   "metadata": {},
   "source": [
    "---\n",
    "# Added user flair as an additional parameter\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3fd7f99-aeef-4ba5-9bda-435ed8b9c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>clean_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Yeah but Kimi's also the most experienced F1 d...</td>\n",
       "      <td>Bwoah</td>\n",
       "      <td>yeah but kimi's also the most experienced f1 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>What is actually happening in the original wit...</td>\n",
       "      <td>Claire Williams is waifu material</td>\n",
       "      <td>what is actually happening in the original wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Yeah insane</td>\n",
       "      <td>BWOAHHHHHHH</td>\n",
       "      <td>yeah insane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>Hamilton did it before at the Germany slip n s...</td>\n",
       "      <td>flairless</td>\n",
       "      <td>hamilton did it before at the germany slip n s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>formuladank</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>flairless</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                               body  \\\n",
       "0  formuladank  Yeah but Kimi's also the most experienced F1 d...   \n",
       "1  formuladank  What is actually happening in the original wit...   \n",
       "2  formuladank                                        Yeah insane   \n",
       "3  formuladank  Hamilton did it before at the Germany slip n s...   \n",
       "4  formuladank                                          [removed]   \n",
       "\n",
       "                   author_flair_text  \\\n",
       "0                              Bwoah   \n",
       "1  Claire Williams is waifu material   \n",
       "2                       BWOAHHHHHHH    \n",
       "3                          flairless   \n",
       "4                          flairless   \n",
       "\n",
       "                                          clean_body  \n",
       "0  yeah but kimi's also the most experienced f1 d...  \n",
       "1  what is actually happening in the original wit...  \n",
       "2                                        yeah insane  \n",
       "3  hamilton did it before at the germany slip n s...  \n",
       "4                                          [removed]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/subreddit_comments.csv')\n",
    "df_t = pd.read_csv('../data/subreddit_titles.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc622122-9e87-448e-9d97-e473164b1d2e",
   "metadata": {},
   "source": [
    "## Starting with comment models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90dde26-cd75-4e72-9b0f-f9aa49eed17a",
   "metadata": {},
   "source": [
    "Column Transformer pulled from [here](https://www.youtube.com/watch?v=HyP5MvlmbRc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58aa4fdb-ae7d-4eb1-843a-0d22d039332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1.0\n",
      "Test: 0.9682436811406351\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'body'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e2e3191-e010-40ab-b612-7e8729756894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1.0\n",
      "Test: 0.9615866388308977\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'title'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df_t)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "\n",
    "X = df_cv\n",
    "y = df_t['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402fb349-41c1-4024-b7b4-7a2d77db3bb2",
   "metadata": {},
   "source": [
    "Wow! Adding the user flair really added to the model. This makes sense because going to each subreddit shows that the flairs are very unique to each subreddit. \n",
    "The only posts or comments that will not have this added benefit are those with no flair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2be1cf03-5d7d-4f6f-8082-213a3da99bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9973350619418035\n",
      "Test: 0.9933030892201339\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'body'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "rf = LogisticRegression()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f077d913-e0c4-4477-87a1-4a03c3e86eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9832180927686546\n",
      "Test: 0.9701879455605963\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'body'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "rf = MultinomialNB()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7631cd58-f941-4bfa-b9cf-603e82ba5367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9456208585422069\n",
      "Test: 0.9466407431410672\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'body'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "rf = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a651562-7f96-40f7-bf6d-012e084c09dd",
   "metadata": {},
   "source": [
    "Looks like Logistic Regression is the winner for the comment submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d9a28fd-83df-483e-a18e-99e1d8d8a94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9969749351771824\n",
      "Test: 0.9930870598401382\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer(stop_words = 'english')\n",
    "ct = make_column_transformer((cvec, 'body'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "# Model with Grid Search params\n",
    "# Pipeline\n",
    "rf = LogisticRegression()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2943d-bba3-4ef5-b012-12dc7ef8bbea",
   "metadata": {},
   "source": [
    "### Post Titles with flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79a61b72-3d39-475e-8f64-1f0c59260d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9837115411388\n",
      "Test: 0.9711899791231733\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'title'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df_t)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df_t['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "rf = LogisticRegression()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da1c9ad7-2596-42ca-95b8-52f16cf4b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1.0\n",
      "Test: 0.9624217118997912\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'title'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df_t)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df_t['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "abf2d409-d7e5-4ecc-9f46-3d8bdb799ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.976611443686482\n",
      "Test: 0.9590814196242171\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'title'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df_t)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df_t['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "rf = MultinomialNB()\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d3f7253-5ca9-4762-ac46-9267024e2bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9404148684393707\n",
      "Test: 0.9377870563674322\n"
     ]
    }
   ],
   "source": [
    "# X and y\n",
    "cvec = CountVectorizer()\n",
    "ct = make_column_transformer((cvec, 'title'), (cvec, 'author_flair_text'))\n",
    "cv = ct.fit_transform(df_t)\n",
    "df_cv = pd.DataFrame(cv.todense(), columns = ct.get_feature_names_out())\n",
    "\n",
    "X = df_cv\n",
    "y = df_t['subreddit']\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                   random_state = 42,\n",
    "                                                   stratify = y)\n",
    "\n",
    "rf = GradientBoostingClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'Train: {rf.score(X_train, y_train)}')\n",
    "print(f'Test: {rf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26eee1-2d87-419c-9185-b5312c2178ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
